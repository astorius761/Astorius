<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LunarLander AI Project</title>
  <link rel="stylesheet" href="style.css">
  <link href="https://fonts.googleapis.com/css2?family=Orbitron&family=Roboto&display=swap" rel="stylesheet">
 
</head>
<body>
<!-- rebuild -->
  <!-- Navigation -->
  <header class="hidden">
    <div class="logo">
      <img src="images/ChatGPT Image May 9, 2025, 01_27_54 PM.png" alt="LunarLander AI Logo" class="logo-img">
    </div>
    <div class="menu-toggle" id="menu-toggle">
      <span></span>
      <span></span>
      <span></span>
    </div>
    <nav>
      <ul>
        <li><a href="#home">Home</a></li>
        <li><a href="#about">About</a></li>
      
        <li><a href="#algorithms">Algorithms</a></li>
        <li><a href="#workflow">Tools</a></li>
        <li><a href="#comparison">Comparison</a></li>
        <li><a href="#results">Results</a></li>
        <li><a href="documents/Project_Report.pdf" class="btn-download" target="_blank">Download Report</a></li>
      </ul>
    </nav>
  </header>

  <!-- Home Section -->
  <section id="home" class="hero">
    <h2 class="fade-in-text"> Mission: AI Lunar Landing </h2>
    <p>Reinforcement Learning Agent trained to land a spacecraft on the moon</p>
    <div class="cta-buttons">
      <a href="#results" class="btn">Watch Demo</a>
      <a href="documents/Project_Report.pdf" class="btn">Download Report</a>
    </div>
  </section>

  <!-- About Section -->
  <section id="about">
    <h2>Mission Log</h2>
    <div class="card">
      <h3>Why this project?</h3>
      <p>Spacecraft landing is one of the most technically challenging aspects of space exploration. From the early days of airbags and parachutes to modern reusable rockets, humanity has continuously evolved its approach. However, many traditional methods still rely on pre-programmed instructions or human control, which struggle in unpredictable conditions. With the rise of artificial intelligence, this project explores how Reinforcement Learning (RL) can offer a powerful alternative an autonomous system that learns to land spacecraft safely and efficiently through experience.</p>
    </div>
    <div class="card">
      <h3>What is LunarLander?</h3>
      <p>A lunar lander is a spacecraft designed to make a controlled descent onto the surface of a planetary body such as the Moon. It uses thrusters, sensors, and control systems to adjust speed and position mid-flight, ensuring a safe touchdown. From historic Apollo landers to today’s reusable rockets, these vehicles represent the cutting edge of space technology. This project simulates the key mechanics of lunar landing, providing a platform for developing smart, autonomous landing systems using AI.</p>
    </div>
    <div class="card">
      <h3>The AI Method</h3>
      <p>Astorios, our intelligent agent, learns to land through Deep Reinforcement Learning (DRL). His training begins with Q-table learning to grasp basic actions, then advances to Deep Q-Networks (DQN) that interpret raw pixel images. To handle more complex decisions, we implement Proximal Policy Optimization (PPO) a stable and efficient method ideal for high-dimensional tasks. Powered by a Convolutional Neural Network (CNN), Astorios analyzes visual data from the environment and adapts his strategy over time, learning to land safely under ever-changing conditions.</p>
    </div>
    <div class="card">
      <h3>Objectives</h3>
   
      <p>
        This project aims to simulate realistic spacecraft landings using visual inputs and develop an AI agent capable of mastering this task through reinforcement learning. We train <strong>Astorios</strong> using three core techniques—<em>Q-table</em>, <em>Deep Q-Networks (DQN)</em>, and <em>Proximal Policy Optimization (PPO)</em>—to evaluate and compare their performance in a challenging environment. By visualizing the learning process and results, we demonstrate how AI can effectively solve real-world control problems without manual programming. Ultimately, this work highlights the potential of reinforcement learning in advancing autonomous systems for aerospace and beyond.
      </p>
     
    </div>
  </section>



  
<!-- Algorithms Section -->
<section id="algorithms">
  <h2>Algorithms Used</h2>
  
  <div class="algorithm-cards">
    <div class="card">
      <h3>Q-Table</h3>
      <p>A simple reinforcement learning method where the agent learns optimal actions using a table of state-action values.
Here is the reward graph showing the agent’s learning progress with the Q-Table.



</p>
      <br>
      <br>
      <img src="images/photo_2025-05-09_12-35-44.jpg" alt="Q-Table Diagram" class="zoomable">
    
    </div>
    
    <div class="card">
      <h3>Deep Q-Learning</h3>
      <p>An advanced version of Q-learning that replaces the Q-table with a deep neural network, enabling the agent to handle high-dimensional or continuous state spaces like images.
Here is the reward graph showing the learning progression with Deep Q-Learning.</p>
      <br>
      <br>
      <img src="images/original_and_smoothed_rewards_graph.png" alt="Deep Q-Learning Diagram" class="zoomable">
     
    </div>
    
    <div class="card">
      <h3>PPO (Proximal Policy Optimization)</h3>
      <p>A robust policy optimization algorithm that ensures stable and efficient training by controlling how much the policy can change in each update.
Here is the reward graph reflecting the training performance using PPO.</p>
      <br>
      
      <img src="images/photo_2025-05-09_12-36-04.jpg" alt="PPO Diagram" class="zoomable">
     
    </div>
  </div>
</section>

  <!-- Workflow Section -->
  <section id="workflow">
    <h2>Tools</h2>
    <div class="workflow-steps">
      <div class="step">Pytorch</div>
      <div class="step">Google Colab</div>
      <div class="step">Jupyter Notebook</div>
      <div class="step">OpenAI Gym (LunarLander)</div>
      <div class="step">Convolutional Neural Networks (CNNs)</div>
      <div class="step">State: Pixel Capture & Preprocessing</div>
      <div class="step">NumPy & OpenCV</div>
      <div class="step">Experience Replay Buffer</div>
      <div class="step">Target Network Updates</div>
      <div class="step">Landing Success Metrics</div>
    </div>
  </section>
<!-- Algorithm Comparison Section -->
<section id="comparison">
  <h2>Algorithm Comparison</h2>
  
  <div class="comparison-tables">
    <div class="card">
      <h3>Performance Metrics</h3>
      <table>
        <thead>
          <tr>
            <th>Algorithm</th>
            <th>Performance</th>
            <th>Stability</th>
            <th>Convergence</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>PPO</td>
            <td>Strong learning curve</td>
            <td>Stabilizes over time</td>
            <td>Positive rewards</td>
          </tr>
          <tr>
            <td>Q-Table</td>
            <td>Poor, declines over time</td>
            <td>Very unstable</td>
            <td>No convergence</td>
          </tr>
          <tr>
            <td>Deep Q-Learning</td>
            <td>Stable but suboptimal</td>
            <td>Consistently stable</td>
            <td>Suboptimal solution</td>
          </tr>
        </tbody>
      </table>
    </div>
    
    <div class="card">
      <h3>Training Results</h3>
      <table>
        <thead>
          <tr>
            <th>Algorithm</th>
            <th>Episodes</th>
            <th>Initial Rewards</th>
            <th>Final Rewards</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>PPO</td>
            <td>~1500</td>
            <td>Poor (~200 to -600)</td>
            <td>Positive (~0 to 200)</td>
          </tr>
          <tr>
            <td>Q-Table</td>
            <td>~10,000</td>
            <td>Poor (~200 to -600)</td>
            <td>Declining (~200 to -600)</td>
          </tr>
          <tr>
            <td>Deep Q-Learning</td>
            <td>~1300</td>
            <td>Moderate (~50 to -100)</td>
            <td>Stable (~50 to -100)</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

  
  <!-- Results Section -->
  <section id="results">
    <h2>Results</h2>
    <div class="video-container">
      <video src="vid/results.mp4" controls></video>
    </div>
   
  </section>

  <!-- Footer -->
  <footer>
    <p>Made by Lyna Bouharoun| © 2025</p>
    <a href="https://github.com/astorius761" target="_blank">GitHub</a> |
    <a href="mailto:astorius761@gmail.com">Contact</a>
  </footer>

  <div class="mobile-nav" id="mobile-nav">
    <button class="close-btn" id="close-btn">&times;</button>
    
    <ul>
      <li><a href="#home">Home</a></li>
      <li><a href="#about">About</a></li>
     
      <li><a href="#algorithms">Algorithms</a></li>
       <li><a href="#workflow">Tools</a></li>
      <li><a href="#comparison">Comparison</a></li>
      <li><a href="#results">Results</a></li>

    </ul>
  </div>

  <script src="script.js"></script>

</body>
</html>
